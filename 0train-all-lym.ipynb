{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1004f676-247a-4efd-9259-6c7550a1bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import generate_data_list, SdDataset\n",
    "from net import AllSwin_T\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbd8062e-4ff8-4f6e-b1f3-584123d9ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/home/lym/桌面/gzr-lym/dataset\"\n",
    "classes_list = ['0', '1']\n",
    "model_name = \"all_swin_transformer\"\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "341ffc12-13e8-4f69-9827-962fbb77bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b5d8a6c-745e-46d0-a04c-86177c0b0b3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m model_ft \u001b[38;5;241m=\u001b[39m AllSwin_T(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_ft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-py38/lib/python3.8/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-py38/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-py38/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-py38/lib/python3.8/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-py38/lib/python3.8/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# 读取训练数据路径，验证数据路径\n",
    "train_image_path_list, train_label_list = generate_data_list(data_root, classes_list, mode=\"train\")\n",
    "val_image_path_list, val_label_list = generate_data_list(data_root, classes_list, mode=\"test\")\n",
    "\n",
    "train_dataset = SdDataset(train_image_path_list, train_label_list, transform)\n",
    "val_dataset = SdDataset(val_image_path_list, val_label_list, transform)\n",
    "\n",
    "train_dl = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)  # 训练集标签\n",
    "test_dl = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)  # 测试集标签\n",
    "\n",
    "# 判断是否使用GPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = AllSwin_T(num_classes=2)\n",
    "\n",
    "model = model_ft.to(DEVICE)  # 将模型迁移到gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbeeeae3-b51e-4c37-9ab5-d1b73427d61b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m loss_fn\u001b[38;5;241m.\u001b[39mto(DEVICE)  \u001b[38;5;66;03m# 将loss_fn迁移到GPU\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Adam损失函数\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m      6\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(log_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m'\u001b[39m, model_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_logs\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(epoch):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 优化器\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = loss_fn.to(DEVICE)  # 将loss_fn迁移到GPU\n",
    "# Adam损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "writer = SummaryWriter(log_dir=os.path.join('checkpoint', model_name, 'tf_logs'))\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_acc = 0\n",
    "    train_loss = 0\n",
    "    total_loss = 0\n",
    "    # 开始迭代每个batch中的数据\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dl):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 计算损失\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        # 计算准确率\n",
    "        train_acc = correct / total\n",
    "        train_loss = total_loss / total\n",
    "        # 每训练100个batch打印一次训练集的loss和准确率\n",
    "        if (batch_idx + 1) % 5 == 0:\n",
    "            print('Epoch-{}-Batch-{}: Train: Loss-{:.4f}, Accuracy-{:.4f}'.format(epoch + 1,\n",
    "                                                                                  batch_idx + 1,\n",
    "                                                                                  train_loss,\n",
    "                                                                                  train_acc))\n",
    "    # 计算每个epoch内训练集的acc\n",
    "    total_train_acc.append(train_acc), total_train_loss.append(train_loss)\n",
    "    writer.add_scalar('Train/Loss', train_loss, epoch)\n",
    "    writer.add_scalar('Train/Accuracy', train_acc, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcc8f149-f3bf-4881-8b5b-e431dffac731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "def test(epoch, ckpt):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_acc = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_dl):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            test_acc = correct / total\n",
    "            test_loss = total_loss / total\n",
    "        print(\n",
    "            'Epoch-{}-Test Accurancy: {:.3f}'.format(epoch + 1, test_acc), '\\n')\n",
    "\n",
    "    total_test_acc.append(test_acc), total_test_loss.append(test_loss)\n",
    "    writer.add_scalar('Test/Loss', test_loss, epoch)\n",
    "    writer.add_scalar('Test/Accuracy', test_acc, epoch)\n",
    "\n",
    "    # 保存权重文件\n",
    "    acc = 100. * correct / total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        if not os.path.isdir(os.path.join('checkpoint', model_name)):\n",
    "            os.mkdir(os.path.join('checkpoint', model_name))\n",
    "        torch.save(model, os.path.join('checkpoint', model_name, model_name + \"_\" + ckpt))\n",
    "        best_acc = acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d1fb70-f447-4049-9ec0-113ffa33a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_acc = []\n",
    "total_train_acc = []\n",
    "total_test_loss = []\n",
    "total_train_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "026411bd-a891-4686-9abb-dd78603b5ba7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m best_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtrain\u001b[49m(epoch)\n\u001b[1;32m      6\u001b[0m     test(epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# 动态调整学习率\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "epoch = 10\n",
    "best_acc = 0\n",
    "for epoch in range(epoch):\n",
    "    train(epoch)\n",
    "    test(epoch, \"best.pth\")\n",
    "    # 动态调整学习率\n",
    "    optimizer.step()\n",
    "\n",
    "writer.close()\n",
    "plt.figure()\n",
    "plt.plot(range(epoch + 1), total_train_acc, label='Train Accurancy')\n",
    "plt.plot(range(epoch + 1), total_test_acc, label='Test Accurancy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accurancy')\n",
    "plt.title('Accurancy')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join('checkpoint', model_name, f'{model_name}_Accurancy.jpg'))  # 自动保存plot出来的图片\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(epoch + 1), total_train_loss, label='Train Loss')\n",
    "plt.plot(range(epoch + 1), total_test_loss, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join('checkpoint', model_name, f'{model_name}_Loss.jpg'))  # 自动保存plot出来的图片\n",
    "\n",
    "# 输出best_acc\n",
    "print(f'Best Acc: {best_acc}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa856c8-6d7f-4f7b-946f-fa66460428bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cee14b-3835-4ccd-8a88-728703f7de15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92978bdc-958e-4ed0-b4da-fba2a68194c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2aef9b-f18e-4b5b-96d2-0643fce4c428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
