{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "825c6567-dd41-4e3f-b2ab-8a27b547bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import generate_data_list, SdDataset\n",
    "import timm\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "data_root = \"/home/lym/桌面/gzr-lym/dataset\"\n",
    "classes_list = ['0', '1']\n",
    "model_name = \"inception_v3\"\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# 读取训练数据路径，验证数据路径\n",
    "train_image_path_list, train_label_list = generate_data_list(data_root, classes_list, mode=\"train\")\n",
    "val_image_path_list, val_label_list = generate_data_list(data_root, classes_list, mode=\"test\")\n",
    "\n",
    "train_dataset = SdDataset(train_image_path_list, train_label_list, transform)\n",
    "val_dataset = SdDataset(val_image_path_list, val_label_list, transform)\n",
    "\n",
    "train_dl = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)  # 训练集标签\n",
    "test_dl = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)  # 测试集标签\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "984042ca-c35b-4935-a48f-e61a0c8fe3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3414829b-0dcb-48be-81cb-b6f78789a6e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.67 GiB total capacity; 528.88 MiB already allocated; 9.69 MiB free; 546.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 99\u001b[0m\n\u001b[1;32m     97\u001b[0m best_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[0;32m---> 99\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     test(epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# 动态调整学习率\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     23\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(DEVICE), targets\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[1;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-py38/lib/python3.8/site-packages/timm/models/inception_v3.py:385\u001b[0m, in \u001b[0;36mInceptionV3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    383\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, aux\n\u001b[0;32m--> 385\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-py38/lib/python3.8/site-packages/timm/models/inception_v3.py:371\u001b[0m, in \u001b[0;36mInceptionV3.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    369\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_postaux(x)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, aux\n\u001b[0;32m--> 371\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_postaux\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-py38/lib/python3.8/site-packages/timm/models/inception_v3.py:361\u001b[0m, in \u001b[0;36mInceptionV3.forward_postaux\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_postaux\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    360\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMixed_7a(x)  \u001b[38;5;66;03m# N x 1280 x 8 x 8\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMixed_7b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# N x 2048 x 8 x 8\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMixed_7c(x)  \u001b[38;5;66;03m# N x 2048 x 8 x 8\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-py38/lib/python3.8/site-packages/timm/models/inception_v3.py:206\u001b[0m, in \u001b[0;36mInceptionE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    205\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(x)\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.67 GiB total capacity; 528.88 MiB already allocated; 9.69 MiB free; 546.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# 判断是否使用GPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = timm.create_model(model_name=\"inception_v3\", num_classes=2)\n",
    "\n",
    "model = model_ft.to(DEVICE)  # 将模型迁移到gpu\n",
    "\n",
    "# 优化器\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = loss_fn.to(DEVICE)  # 将loss_fn迁移到GPU\n",
    "# Adam损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "writer = SummaryWriter(log_dir=os.path.join('checkpoint', model_name, 'tf_logs'))\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_acc = 0\n",
    "    train_loss = 0\n",
    "    total_loss = 0\n",
    "    # 开始迭代每个batch中的数据\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dl):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 计算损失\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        # 计算准确率\n",
    "        train_acc = correct / total\n",
    "        train_loss = total_loss / total\n",
    "        # 每训练100个batch打印一次训练集的loss和准确率\n",
    "        if (batch_idx + 1) % 5 == 0:\n",
    "            print('Epoch-{}-Batch-{}: Train: Loss-{:.4f}, Accuracy-{:.4f}'.format(epoch + 1,\n",
    "                                                                                  batch_idx + 1,\n",
    "                                                                                  train_loss,\n",
    "                                                                                  train_acc))\n",
    "\n",
    "    # 计算每个epoch内训练集的acc\n",
    "    total_train_acc.append(train_acc), total_train_loss.append(train_loss)\n",
    "    writer.add_scalar('Train/Loss', train_loss, epoch)\n",
    "    writer.add_scalar('Train/Accuracy', train_acc, epoch)\n",
    "\n",
    "\n",
    "# Testing\n",
    "def test(epoch, ckpt):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_acc = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_dl):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            test_acc = correct / total\n",
    "            test_loss = total_loss / total\n",
    "        print(\n",
    "            'Epoch-{}-Test Accurancy: {:.3f}'.format(epoch + 1, test_acc), '\\n')\n",
    "\n",
    "    total_test_acc.append(test_acc), total_test_loss.append(test_loss)\n",
    "    writer.add_scalar('Test/Loss', test_loss, epoch)\n",
    "    writer.add_scalar('Test/Accuracy', test_acc, epoch)\n",
    "\n",
    "    # 保存权重文件\n",
    "    acc = 100. * correct / total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        if not os.path.isdir(os.path.join('checkpoint', model_name)):\n",
    "            os.mkdir(os.path.join('checkpoint', model_name))\n",
    "        torch.save(model, os.path.join('checkpoint', model_name, model_name + \"_\" + ckpt))\n",
    "        best_acc = acc\n",
    "\n",
    "\n",
    "total_test_acc = []\n",
    "total_train_acc = []\n",
    "total_test_loss = []\n",
    "total_train_loss = []\n",
    "# 开始训练\n",
    "epoch = 20\n",
    "best_acc = 0\n",
    "for epoch in range(epoch):\n",
    "    train(epoch)\n",
    "    test(epoch, \"best.pth\")\n",
    "    # 动态调整学习率\n",
    "    optimizer.step()\n",
    "\n",
    "writer.close()\n",
    "plt.figure()\n",
    "plt.plot(range(epoch + 1), total_train_acc, label='Train Accurancy')\n",
    "plt.plot(range(epoch + 1), total_test_acc, label='Test Accurancy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accurancy')\n",
    "plt.title('Accurancy')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join('checkpoint', model_name, f'{model_name}_Accurancy.jpg'))  # 自动保存plot出来的图片\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(epoch + 1), total_train_loss, label='Train Loss')\n",
    "plt.plot(range(epoch + 1), total_test_loss, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join('checkpoint', model_name, f'{model_name}_Loss.jpg'))  # 自动保存plot出来的图片\n",
    "\n",
    "# 输出best_acc\n",
    "print(f'Best Acc: {best_acc}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e53aa1-175d-4d71-b6b7-74a8ec9917b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
